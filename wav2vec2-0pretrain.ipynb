{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"widgets":{"application/vnd.jupyter.widget-state+json":{"fe46e963445a49a78128c50237f5a71a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5a1b571d50754b0c9b579f60ea618a90","IPY_MODEL_6212e85486c2417f92dd9c1d09825bce","IPY_MODEL_e6cd8edc064b4ce0888615b5e6a5af22"],"layout":"IPY_MODEL_19cb45e6f9ee463bbf16a9fe5949fef1"}},"5a1b571d50754b0c9b579f60ea618a90":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5b0ac82a60844a66961385c6273856b7","placeholder":"​","style":"IPY_MODEL_c73fa17621ff477dbd49805a98dfe0e2","value":"Generating train split: "}},"6212e85486c2417f92dd9c1d09825bce":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_cb256ee161c040659eb5d4cc3b03e3ca","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a00573af519044ecb7b83d15137ba2db","value":1}},"e6cd8edc064b4ce0888615b5e6a5af22":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1d9e9df3de614577add936a53e550da4","placeholder":"​","style":"IPY_MODEL_8d5baa342d4e4302b5e4a9a68f737b0b","value":" 2187/0 [00:55&lt;00:00, 42.57 examples/s]"}},"19cb45e6f9ee463bbf16a9fe5949fef1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5b0ac82a60844a66961385c6273856b7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c73fa17621ff477dbd49805a98dfe0e2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cb256ee161c040659eb5d4cc3b03e3ca":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"a00573af519044ecb7b83d15137ba2db":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1d9e9df3de614577add936a53e550da4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8d5baa342d4e4302b5e4a9a68f737b0b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11731549,"sourceType":"datasetVersion","datasetId":7364508},{"sourceId":11735965,"sourceType":"datasetVersion","datasetId":7367573},{"sourceId":11772360,"sourceType":"datasetVersion","datasetId":7361903}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install datasets librosa","metadata":{"id":"F01lefEGx6pQ","trusted":true,"execution":{"iopub.status.busy":"2025-05-11T16:54:10.771823Z","iopub.execute_input":"2025-05-11T16:54:10.772036Z","iopub.status.idle":"2025-05-11T16:54:15.624859Z","shell.execute_reply.started":"2025-05-11T16:54:10.772019Z","shell.execute_reply":"2025-05-11T16:54:15.623874Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.5.0)\nRequirement already satisfied: librosa in /usr/local/lib/python3.11/dist-packages (0.10.2.post1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.3)\nRequirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\nRequirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\nCollecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets)\n  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.16)\nRequirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.30.2)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\nRequirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.11/dist-packages (from librosa) (3.0.1)\nRequirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.15.2)\nRequirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.2.2)\nRequirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.4.2)\nRequirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (4.4.2)\nRequirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.60.0)\nRequirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.13.1)\nRequirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.8.2)\nRequirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.5.0.post1)\nRequirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (4.13.1)\nRequirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.4)\nRequirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.1.0)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.2.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.19.0)\nRequirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51.0->librosa) (0.43.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (2.4.1)\nRequirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa) (4.3.7)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.20.0->librosa) (3.6.0)\nRequirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile>=0.12.1->librosa) (1.17.1)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.22)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->datasets) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->datasets) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->datasets) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->datasets) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->datasets) (2024.2.0)\nDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: fsspec\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 2025.3.2\n    Uninstalling fsspec-2025.3.2:\n      Successfully uninstalled fsspec-2025.3.2\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.12.0 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.8.4.1 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.3.3.83 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.9.90 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.7.3.90 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.8.93 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.8.93 which is incompatible.\nbigframes 1.36.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed fsspec-2024.12.0\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import argparse\nimport math\nimport os\nimport random\nfrom dataclasses import dataclass\nfrom pathlib import Path\nfrom typing import Optional, Union\n\nimport datasets\nimport torch\nfrom accelerate import Accelerator\nfrom accelerate.logging import get_logger\nfrom datasets import DatasetDict, concatenate_datasets, load_dataset\nfrom huggingface_hub import HfApi\nfrom torch.utils.data.dataloader import DataLoader\nfrom tqdm.auto import tqdm\n\nimport transformers\nfrom transformers import (\n    SchedulerType,\n    Wav2Vec2Config,\n    Wav2Vec2FeatureExtractor,\n    Wav2Vec2ForPreTraining,\n    get_scheduler,\n    is_wandb_available,\n    set_seed,\n)\nfrom transformers.models.wav2vec2.modeling_wav2vec2 import _compute_mask_indices, _sample_negative_indices\nfrom transformers.utils import send_example_telemetry\n\n\nlogger = get_logger(__name__)","metadata":{"id":"1kVMSiynx4ou","trusted":true,"execution":{"iopub.status.busy":"2025-05-11T16:54:45.572771Z","iopub.execute_input":"2025-05-11T16:54:45.573526Z","iopub.status.idle":"2025-05-11T16:55:09.671182Z","shell.execute_reply.started":"2025-05-11T16:54:45.573498Z","shell.execute_reply":"2025-05-11T16:55:09.670407Z"}},"outputs":[{"name":"stderr","text":"2025-05-11 16:54:58.200676: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1746982498.356326      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1746982498.400516      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"from dataclasses import dataclass, field\nfrom transformers import SchedulerType\nimport os\n\n@dataclass\nclass Config:\n    dataset_dir: str = \"/kaggle/input/wav2vec2-0pret/Dataset\"        # ✅ Kaggle dataset path\n    checkpoint_dir: str = \"/kaggle/working/TrainingCheckpoints/training_state.pt\"       # ✅ Kaggle writable dir\n    load_config_path : str = \"/kaggle/working/TrainingCheckpoints/iter_index.txt\"\n    bengali_tokenizer: str = \"tanmoyio/wav2vec2-large-xlsr-bengali\"\n    epoch_per_chunk: int = 1\n    model_name_or_path: str = \"patrickvonplaten/wav2vec2-base-v2\" # Facebook/wav2vec2\n    trust_remote_code: bool = True\n    preprocessing_num_workers: int = None\n    overwrite_cache: bool = False\n    preprocessing_only: bool = False\n    cache_dir: str = None\n    validation_split_percentage: int = 20\n    logging_steps: int = 1\n    saving_steps: int = 10000\n    audio_column_name: str = \"audio\"\n    config_name: str = None\n    train_cache_file_name: str = None\n    validation_cache_file_name: str = None\n    per_device_train_batch_size: int = 8\n    per_device_eval_batch_size: int = 8\n    learning_rate: float = 0.001\n    weight_decay: float = 0.01\n    max_train_steps: int = 32\n    gradient_accumulation_steps: int = 4\n    gradient_checkpointing: bool = True\n    lr_scheduler_type: SchedulerType = \"linear\"\n    num_warmup_steps: int = 32000\n    output_dir: str = \"./wav2vec2-pretrained-demo\"\n    seed: int = 0\n    max_gumbel_temperature: float = 2.0\n    min_gumbel_temperature: float = 0.5\n    gumbel_temperature_decay: float = 0.999995\n    max_duration_in_seconds: float = 20.0\n    min_duration_in_seconds: float = 2.0\n    pad_to_multiple_of: int = None\n    adam_beta1: float = 0.9\n    adam_beta2: float = 0.98\n    adam_epsilon: float = 1e-6\n    push_to_hub: bool = False\n    hub_model_id: str = None\n    hub_token: str = None\n    mask_time_prob: float = 0.65\n    mask_time_length: int = 10\n\n\nconfig = Config()","metadata":{"id":"RWzp1ASGyJYN","trusted":true,"execution":{"iopub.status.busy":"2025-05-11T17:07:35.696940Z","iopub.execute_input":"2025-05-11T17:07:35.697411Z","iopub.status.idle":"2025-05-11T17:07:35.706593Z","shell.execute_reply.started":"2025-05-11T17:07:35.697390Z","shell.execute_reply":"2025-05-11T17:07:35.705806Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"# Define the path to the iter_index.txt file in a writable directory\niter_index_path = \"/kaggle/working/TrainingCheckpoints/iter_index.txt\"\n\n# Check if the file exists and read the contents\nif os.path.exists(iter_index_path):\n    with open(iter_index_path, \"r\") as f:\n        iter_index = f.read().strip()  # Read and strip any extra spaces or newlines\n    print(f\"Contents of iter_index.txt: {iter_index}\")\nelse:\n    print(f\"ℹ️ [LOAD] iter_index.txt not found at {iter_index_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T19:13:20.434158Z","iopub.execute_input":"2025-05-11T19:13:20.434413Z","iopub.status.idle":"2025-05-11T19:13:20.439749Z","shell.execute_reply.started":"2025-05-11T19:13:20.434397Z","shell.execute_reply":"2025-05-11T19:13:20.439061Z"}},"outputs":[{"name":"stdout","text":"Contents of iter_index.txt: 35\n","output_type":"stream"}],"execution_count":309},{"cell_type":"code","source":"if os.path.exists(config.load_config_path):\n    with open(config.load_config_path, \"r\") as f:\n        iter_index = int(f.read().strip())\nelse:\n    iter_index = 0\n\nprint(f\"❇️ [LOAD INFO] Iteration Index: {iter_index}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T19:13:22.361732Z","iopub.execute_input":"2025-05-11T19:13:22.361986Z","iopub.status.idle":"2025-05-11T19:13:22.367295Z","shell.execute_reply.started":"2025-05-11T19:13:22.361967Z","shell.execute_reply":"2025-05-11T19:13:22.366511Z"}},"outputs":[{"name":"stdout","text":"❇️ [LOAD INFO] Iteration Index: 35\n","output_type":"stream"}],"execution_count":310},{"cell_type":"code","source":"from dataclasses import dataclass\nfrom typing import Optional, Union\nimport torch\nfrom transformers import Wav2Vec2ForPreTraining, Wav2Vec2FeatureExtractor\nfrom transformers.models.wav2vec2.modeling_wav2vec2 import _compute_mask_indices, _sample_negative_indices\n\n\n@dataclass\nclass DataCollatorForWav2Vec2Pretraining:\n    \"\"\"\n    Data collator for dynamic padding and generating masked indices for wav2vec2 pretraining.\n    \"\"\"\n    model: Wav2Vec2ForPreTraining\n    feature_extractor: Wav2Vec2FeatureExtractor\n    config: Config  # <- use config for relevant parameters\n\n    def __call__(self, features: list[dict[str, Union[list[int], torch.Tensor]]]) -> dict[str, torch.Tensor]:\n        # reformat list to dict and set to pytorch format\n        batch = self.feature_extractor.pad(\n            features,\n            padding=\"longest\",  # From original class; can also be `self.config.padding_strategy` if desired\n            pad_to_multiple_of=self.config.pad_to_multiple_of,\n            return_tensors=\"pt\",\n        )\n\n        device = batch[\"input_values\"].device\n        batch_size = batch[\"input_values\"].shape[0]\n\n        mask_indices_seq_length = self.model._get_feat_extract_output_lengths(batch[\"input_values\"].shape[-1])\n        mask_indices_seq_length = int(mask_indices_seq_length)\n\n        # print(f\"Inside data collator. Mask_time_prob: {self.config.mask_time_prob}\")\n\n        if batch.get(\"attention_mask\") is not None:\n            batch[\"sub_attention_mask\"] = self.model._get_feature_vector_attention_mask(\n                mask_indices_seq_length, batch[\"attention_mask\"]\n            )\n\n        features_shape = (batch_size, mask_indices_seq_length)\n\n        mask_time_indices = _compute_mask_indices(\n            features_shape,\n            self.config.mask_time_prob,\n            self.config.mask_time_length,\n            attention_mask=batch.get(\"sub_attention_mask\"),\n        )\n\n        sampled_negative_indices = _sample_negative_indices(\n            features_shape,\n            self.model.config.num_negatives,\n            mask_time_indices=mask_time_indices,\n        )\n\n        batch[\"mask_time_indices\"] = torch.tensor(mask_time_indices, dtype=torch.long, device=device)\n        batch[\"sampled_negative_indices\"] = torch.tensor(sampled_negative_indices, dtype=torch.long, device=device)\n\n        return batch","metadata":{"id":"n_pVL8Lw1bPf","trusted":true,"execution":{"iopub.status.busy":"2025-05-11T19:13:24.333598Z","iopub.execute_input":"2025-05-11T19:13:24.333865Z","iopub.status.idle":"2025-05-11T19:13:24.343229Z","shell.execute_reply.started":"2025-05-11T19:13:24.333846Z","shell.execute_reply":"2025-05-11T19:13:24.342399Z"}},"outputs":[],"execution_count":311},{"cell_type":"code","source":"def multiply_grads(params, c):\n    \"\"\"Multiplies grads by a constant *c*.\"\"\"\n    for p in params:\n        if p.grad is not None:\n            if torch.is_tensor(c):\n                c = c.to(p.grad.device)\n            p.grad.data.mul_(c)\n\n\ndef get_grad_norm(params, scale=1):\n    \"\"\"Compute grad norm given a gradient scale.\"\"\"\n    total_norm = 0.0\n    for p in params:\n        if p.grad is not None:\n            param_norm = (p.grad.detach().data / scale).norm(2)\n            total_norm += param_norm.item() ** 2\n    total_norm = total_norm**0.5\n    return total_norm","metadata":{"id":"6wlVNkby1pI-","trusted":true,"execution":{"iopub.status.busy":"2025-05-11T19:13:27.280947Z","iopub.execute_input":"2025-05-11T19:13:27.281181Z","iopub.status.idle":"2025-05-11T19:13:27.286224Z","shell.execute_reply.started":"2025-05-11T19:13:27.281164Z","shell.execute_reply":"2025-05-11T19:13:27.285542Z"}},"outputs":[],"execution_count":312},{"cell_type":"code","source":"import os\nimport logging\nfrom transformers import is_wandb_available\nfrom accelerate import Accelerator\n\n# Initialize the Accelerator\naccelerator = Accelerator()\n\n# Setup logger\nlogger = logging.getLogger(__name__)\nlogging.basicConfig(level=logging.INFO)\n\n# W&B logic\nif accelerator.is_local_main_process:\n    datasets.utils.logging.set_verbosity_warning()\n    transformers.utils.logging.set_verbosity_info()\n    if 'KAGGLE_KERNEL_RUN_TYPE' not in os.environ and is_wandb_available():  # Check if running outside Kaggle\n        try:\n            import wandb\n            # Initialize W&B in offline mode (logs locally without needing internet)\n            wandb.init(project=config.output_dir.split(\"/\")[-1], mode=\"offline\")\n            logger.info(\"W&B initialized in offline mode.\")\n        except Exception as e:\n            logger.error(f\"Error initializing W&B: {e}\")\n    else:\n        logger.info(\"ℹ️ [INFO] W&B is not available or skipping W&B in Kaggle. Using local logging.\")\nelse:\n    datasets.utils.logging.set_verbosity_error()\n    transformers.utils.logging.set_verbosity_error()\n\n# Check if the seed is set in the config and set it\nif config.seed is not None:\n    set_seed(config.seed)\n\n# Check and create output directory if necessary\nif accelerator.is_main_process:\n    if config.push_to_hub and not config.preprocessing_only:\n        repo_name = config.hub_model_id or Path(config.output_dir).absolute().name\n        api = HfApi()\n        repo_id = api.create_repo(repo_name, exist_ok=True, token=config.hub_token).repo_id\n        with open(os.path.join(config.output_dir, \".gitignore\"), \"w+\") as gitignore:\n            if \"step_*\" not in gitignore:\n                gitignore.write(\"step_*\\n\")\n            if \"epoch_*\" not in gitignore:\n                gitignore.write(\"epoch_*\\n\")\n    elif config.output_dir is not None:\n        os.makedirs(config.output_dir, exist_ok=True)\n\naccelerator.wait_for_everyone()\n\n# Rest of your code continues here...\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T19:13:29.073972Z","iopub.execute_input":"2025-05-11T19:13:29.074539Z","iopub.status.idle":"2025-05-11T19:13:29.084963Z","shell.execute_reply.started":"2025-05-11T19:13:29.074518Z","shell.execute_reply":"2025-05-11T19:13:29.084411Z"}},"outputs":[],"execution_count":313},{"cell_type":"code","source":"chunk_path = f\"{config.dataset_dir}/dataset_chunk_{iter_index}.parquet\"\n\nassert os.path.exists(chunk_path), f\"Missing dataset chunk: {chunk_path}\"\ndataset = load_dataset(\"parquet\", data_files={'train': chunk_path})\nprint(f\"❇️ [LOAD INFO] Loaded data from {chunk_path}\")\n\n\ndataset = dataset.remove_columns(['path', 'labels'])\n\n# Split into train and validation\ntrain_valid_split = dataset['train'].train_test_split(test_size=187, shuffle=True, seed=42)\n\ndataset = DatasetDict({\n    'train': train_valid_split['train'],\n    'validation': train_valid_split['test']\n})\n\ntrain_dataset = dataset['train']\neval_dataset = dataset['validation']","metadata":{"id":"sLLBpDhOk3sz","outputId":"94f48950-7af6-42a2-9fb5-4d94b5527c8b","trusted":true,"execution":{"iopub.status.busy":"2025-05-11T19:13:31.614643Z","iopub.execute_input":"2025-05-11T19:13:31.614907Z","iopub.status.idle":"2025-05-11T19:13:45.532011Z","shell.execute_reply.started":"2025-05-11T19:13:31.614888Z","shell.execute_reply":"2025-05-11T19:13:45.531274Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"391e71ddb45d42d2b2070dbd40de0a1a"}},"metadata":{}},{"name":"stdout","text":"❇️ [LOAD INFO] Loaded data from /kaggle/input/wav2vec2-0pret/Dataset/dataset_chunk_35.parquet\n","output_type":"stream"}],"execution_count":314},{"cell_type":"code","source":"from datasets import load_dataset, DatasetDict\nimport os, json\nimport numpy as np\nfrom transformers import TrainingArguments, Trainer\nfrom transformers import Wav2Vec2FeatureExtractor, Wav2Vec2Processor, Wav2Vec2CTCTokenizer, Wav2Vec2ForCTC\n\nimport torch\n\nfrom dataclasses import dataclass, field\nfrom typing import Any, Dict, List, Optional, Union","metadata":{"id":"PdxeX2XPwpgy","trusted":true,"execution":{"iopub.status.busy":"2025-05-11T19:13:47.741592Z","iopub.execute_input":"2025-05-11T19:13:47.741864Z","iopub.status.idle":"2025-05-11T19:13:47.746736Z","shell.execute_reply.started":"2025-05-11T19:13:47.741845Z","shell.execute_reply":"2025-05-11T19:13:47.745722Z"}},"outputs":[],"execution_count":315},{"cell_type":"code","source":"from transformers import Wav2Vec2CTCTokenizer\n\n# Specify the local path where the tokenizer files are saved\nlocal_tokenizer_path = \"/kaggle/input/tanmoyio-wav2vec2-0\"  # Replace with actual path\n\n# Load the tokenizer from the local directory\ntokenizer = Wav2Vec2CTCTokenizer.from_pretrained(local_tokenizer_path)\n\n# Verify tokenizer loaded correctly\nprint(\"Tokenizer loaded successfully.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T19:13:49.934619Z","iopub.execute_input":"2025-05-11T19:13:49.934885Z","iopub.status.idle":"2025-05-11T19:13:49.951921Z","shell.execute_reply.started":"2025-05-11T19:13:49.934866Z","shell.execute_reply":"2025-05-11T19:13:49.951291Z"}},"outputs":[{"name":"stderr","text":"loading file vocab.json\nloading file tokenizer_config.json\nloading file added_tokens.json\nloading file special_tokens_map.json\nloading file tokenizer.json\nloading file chat_template.jinja\nloading configuration file /kaggle/input/tanmoyio-wav2vec2-0/config.json\nModel config Wav2Vec2Config {\n  \"activation_dropout\": 0.0,\n  \"adapter_attn_dim\": null,\n  \"adapter_kernel_size\": 3,\n  \"adapter_stride\": 2,\n  \"add_adapter\": false,\n  \"apply_spec_augment\": true,\n  \"architectures\": [\n    \"Wav2Vec2ForCTC\"\n  ],\n  \"attention_dropout\": 0.1,\n  \"bos_token_id\": 1,\n  \"classifier_proj_size\": 256,\n  \"codevector_dim\": 256,\n  \"contrastive_logits_temperature\": 0.1,\n  \"conv_bias\": true,\n  \"conv_dim\": [\n    512,\n    512,\n    512,\n    512,\n    512,\n    512,\n    512\n  ],\n  \"conv_kernel\": [\n    10,\n    3,\n    3,\n    3,\n    3,\n    2,\n    2\n  ],\n  \"conv_stride\": [\n    5,\n    2,\n    2,\n    2,\n    2,\n    2,\n    2\n  ],\n  \"ctc_loss_reduction\": \"mean\",\n  \"ctc_zero_infinity\": true,\n  \"diversity_loss_weight\": 0.1,\n  \"do_stable_layer_norm\": true,\n  \"eos_token_id\": 2,\n  \"feat_extract_activation\": \"gelu\",\n  \"feat_extract_dropout\": 0.0,\n  \"feat_extract_norm\": \"layer\",\n  \"feat_proj_dropout\": 0.0,\n  \"feat_quantizer_dropout\": 0.0,\n  \"final_dropout\": 0.0,\n  \"gradient_checkpointing\": true,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout\": 0.1,\n  \"hidden_size\": 1024,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 4096,\n  \"layer_norm_eps\": 1e-05,\n  \"layerdrop\": 0.1,\n  \"mask_channel_length\": 10,\n  \"mask_channel_min_space\": 1,\n  \"mask_channel_other\": 0.0,\n  \"mask_channel_prob\": 0.0,\n  \"mask_channel_selection\": \"static\",\n  \"mask_feature_length\": 10,\n  \"mask_feature_min_masks\": 0,\n  \"mask_feature_prob\": 0.0,\n  \"mask_time_length\": 10,\n  \"mask_time_min_masks\": 2,\n  \"mask_time_min_space\": 1,\n  \"mask_time_other\": 0.0,\n  \"mask_time_prob\": 0.05,\n  \"mask_time_selection\": \"static\",\n  \"model_type\": \"wav2vec2\",\n  \"num_adapter_layers\": 3,\n  \"num_attention_heads\": 16,\n  \"num_codevector_groups\": 2,\n  \"num_codevectors_per_group\": 320,\n  \"num_conv_pos_embedding_groups\": 16,\n  \"num_conv_pos_embeddings\": 128,\n  \"num_feat_extract_layers\": 7,\n  \"num_hidden_layers\": 24,\n  \"num_negatives\": 100,\n  \"output_hidden_size\": 1024,\n  \"pad_token_id\": 118,\n  \"proj_codevector_dim\": 256,\n  \"tdnn_dilation\": [\n    1,\n    2,\n    3,\n    1,\n    1\n  ],\n  \"tdnn_dim\": [\n    512,\n    512,\n    512,\n    512,\n    1500\n  ],\n  \"tdnn_kernel\": [\n    5,\n    3,\n    3,\n    1,\n    1\n  ],\n  \"transformers_version\": \"4.51.1\",\n  \"use_weighted_layer_sum\": false,\n  \"vocab_size\": 119,\n  \"xvector_output_dim\": 512\n}\n\n","output_type":"stream"},{"name":"stdout","text":"Tokenizer loaded successfully.\n","output_type":"stream"}],"execution_count":316},{"cell_type":"code","source":"tokenizer = Wav2Vec2CTCTokenizer.from_pretrained(\n    \"/kaggle/input/tanmoyio-wav2vec2-0\",  # Local path to the tokenizer directory\n    unk_token=\"[UNK]\",\n    pad_token=\"[PAD]\",\n    word_delimiter_token=\"|\"\n)","metadata":{"id":"PQ-Ga28iwNYy","trusted":true,"execution":{"iopub.status.busy":"2025-05-11T19:13:53.875347Z","iopub.execute_input":"2025-05-11T19:13:53.875612Z","iopub.status.idle":"2025-05-11T19:13:53.899673Z","shell.execute_reply.started":"2025-05-11T19:13:53.875594Z","shell.execute_reply":"2025-05-11T19:13:53.899177Z"}},"outputs":[{"name":"stderr","text":"loading file vocab.json\nloading file tokenizer_config.json\nloading file added_tokens.json\nloading file special_tokens_map.json\nloading file tokenizer.json\nloading file chat_template.jinja\nloading configuration file /kaggle/input/tanmoyio-wav2vec2-0/config.json\nModel config Wav2Vec2Config {\n  \"activation_dropout\": 0.0,\n  \"adapter_attn_dim\": null,\n  \"adapter_kernel_size\": 3,\n  \"adapter_stride\": 2,\n  \"add_adapter\": false,\n  \"apply_spec_augment\": true,\n  \"architectures\": [\n    \"Wav2Vec2ForCTC\"\n  ],\n  \"attention_dropout\": 0.1,\n  \"bos_token_id\": 1,\n  \"classifier_proj_size\": 256,\n  \"codevector_dim\": 256,\n  \"contrastive_logits_temperature\": 0.1,\n  \"conv_bias\": true,\n  \"conv_dim\": [\n    512,\n    512,\n    512,\n    512,\n    512,\n    512,\n    512\n  ],\n  \"conv_kernel\": [\n    10,\n    3,\n    3,\n    3,\n    3,\n    2,\n    2\n  ],\n  \"conv_stride\": [\n    5,\n    2,\n    2,\n    2,\n    2,\n    2,\n    2\n  ],\n  \"ctc_loss_reduction\": \"mean\",\n  \"ctc_zero_infinity\": true,\n  \"diversity_loss_weight\": 0.1,\n  \"do_stable_layer_norm\": true,\n  \"eos_token_id\": 2,\n  \"feat_extract_activation\": \"gelu\",\n  \"feat_extract_dropout\": 0.0,\n  \"feat_extract_norm\": \"layer\",\n  \"feat_proj_dropout\": 0.0,\n  \"feat_quantizer_dropout\": 0.0,\n  \"final_dropout\": 0.0,\n  \"gradient_checkpointing\": true,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout\": 0.1,\n  \"hidden_size\": 1024,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 4096,\n  \"layer_norm_eps\": 1e-05,\n  \"layerdrop\": 0.1,\n  \"mask_channel_length\": 10,\n  \"mask_channel_min_space\": 1,\n  \"mask_channel_other\": 0.0,\n  \"mask_channel_prob\": 0.0,\n  \"mask_channel_selection\": \"static\",\n  \"mask_feature_length\": 10,\n  \"mask_feature_min_masks\": 0,\n  \"mask_feature_prob\": 0.0,\n  \"mask_time_length\": 10,\n  \"mask_time_min_masks\": 2,\n  \"mask_time_min_space\": 1,\n  \"mask_time_other\": 0.0,\n  \"mask_time_prob\": 0.05,\n  \"mask_time_selection\": \"static\",\n  \"model_type\": \"wav2vec2\",\n  \"num_adapter_layers\": 3,\n  \"num_attention_heads\": 16,\n  \"num_codevector_groups\": 2,\n  \"num_codevectors_per_group\": 320,\n  \"num_conv_pos_embedding_groups\": 16,\n  \"num_conv_pos_embeddings\": 128,\n  \"num_feat_extract_layers\": 7,\n  \"num_hidden_layers\": 24,\n  \"num_negatives\": 100,\n  \"output_hidden_size\": 1024,\n  \"pad_token_id\": 118,\n  \"proj_codevector_dim\": 256,\n  \"tdnn_dilation\": [\n    1,\n    2,\n    3,\n    1,\n    1\n  ],\n  \"tdnn_dim\": [\n    512,\n    512,\n    512,\n    512,\n    1500\n  ],\n  \"tdnn_kernel\": [\n    5,\n    3,\n    3,\n    1,\n    1\n  ],\n  \"transformers_version\": \"4.51.1\",\n  \"use_weighted_layer_sum\": false,\n  \"vocab_size\": 119,\n  \"xvector_output_dim\": 512\n}\n\n","output_type":"stream"}],"execution_count":317},{"cell_type":"code","source":"feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(\"/kaggle/input/tanmoyio-wav2vec2-0\")\n\nif not feature_extractor.do_normalize:\n    raise ValueError(\"Feature extractor must normalize inputs (do_normalize=True).\")\n","metadata":{"id":"uQG6MH_bM8o-","trusted":true,"execution":{"iopub.status.busy":"2025-05-11T19:13:58.284901Z","iopub.execute_input":"2025-05-11T19:13:58.285159Z","iopub.status.idle":"2025-05-11T19:13:58.294147Z","shell.execute_reply.started":"2025-05-11T19:13:58.285140Z","shell.execute_reply":"2025-05-11T19:13:58.293430Z"}},"outputs":[{"name":"stderr","text":"loading configuration file /kaggle/input/tanmoyio-wav2vec2-0/preprocessor_config.json\nFeature extractor Wav2Vec2FeatureExtractor {\n  \"do_normalize\": true,\n  \"feature_extractor_type\": \"Wav2Vec2FeatureExtractor\",\n  \"feature_size\": 1,\n  \"padding_side\": \"right\",\n  \"padding_value\": 0.0,\n  \"return_attention_mask\": true,\n  \"sampling_rate\": 16000\n}\n\n","output_type":"stream"}],"execution_count":318},{"cell_type":"code","source":"# Import the necessary class\nfrom transformers import Wav2Vec2Config\n\n# Load the model config from the local path or Hugging Face model\nmodel_config = Wav2Vec2Config.from_pretrained(\"/kaggle/input/wav2vec2-base-v2\")\n\n# Update vocab_size in the config\nmodel_config.vocab_size = tokenizer.vocab_size  # Set the tokenizer's vocab_size in the config\n\nprint(\"Model config loaded and vocab_size updated.\")\n","metadata":{"id":"N0eTjOzJXaEH","trusted":true,"execution":{"iopub.status.busy":"2025-05-11T19:14:00.699939Z","iopub.execute_input":"2025-05-11T19:14:00.700508Z","iopub.status.idle":"2025-05-11T19:14:02.393302Z","shell.execute_reply.started":"2025-05-11T19:14:00.700480Z","shell.execute_reply":"2025-05-11T19:14:02.392581Z"}},"outputs":[{"name":"stderr","text":"loading configuration file /kaggle/input/wav2vec2-base-v2/config.json\nModel config Wav2Vec2Config {\n  \"activation_dropout\": 0.0,\n  \"adapter_attn_dim\": null,\n  \"adapter_kernel_size\": 3,\n  \"adapter_stride\": 2,\n  \"add_adapter\": false,\n  \"apply_spec_augment\": true,\n  \"architectures\": [\n    \"Wav2Vec2ForPreTraining\"\n  ],\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 1,\n  \"classifier_proj_size\": 256,\n  \"codevector_dim\": 256,\n  \"contrastive_logits_temperature\": 0.1,\n  \"conv_bias\": true,\n  \"conv_dim\": [\n    512,\n    512,\n    512,\n    512,\n    512,\n    512,\n    512\n  ],\n  \"conv_kernel\": [\n    10,\n    3,\n    3,\n    3,\n    3,\n    2,\n    2\n  ],\n  \"conv_stride\": [\n    5,\n    2,\n    2,\n    2,\n    2,\n    2,\n    2\n  ],\n  \"ctc_loss_reduction\": \"sum\",\n  \"ctc_zero_infinity\": false,\n  \"diversity_loss_weight\": 0.1,\n  \"do_stable_layer_norm\": true,\n  \"eos_token_id\": 2,\n  \"feat_extract_activation\": \"gelu\",\n  \"feat_extract_dropout\": 0.0,\n  \"feat_extract_norm\": \"layer\",\n  \"feat_proj_dropout\": 0.0,\n  \"feat_quantizer_dropout\": 0.0,\n  \"final_dropout\": 0.0,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout\": 0.0,\n  \"hidden_dropout_prob\": 0.0,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-05,\n  \"layerdrop\": 0.0,\n  \"mask_feature_length\": 10,\n  \"mask_feature_min_masks\": 0,\n  \"mask_feature_prob\": 0.0,\n  \"mask_time_length\": 10,\n  \"mask_time_min_masks\": 2,\n  \"mask_time_prob\": 0.65,\n  \"model_type\": \"wav2vec2\",\n  \"num_adapter_layers\": 3,\n  \"num_attention_heads\": 12,\n  \"num_codevector_groups\": 2,\n  \"num_codevectors_per_group\": 320,\n  \"num_conv_pos_embedding_groups\": 16,\n  \"num_conv_pos_embeddings\": 128,\n  \"num_feat_extract_layers\": 7,\n  \"num_hidden_layers\": 12,\n  \"num_negatives\": 100,\n  \"output_hidden_size\": 768,\n  \"pad_token_id\": 0,\n  \"proj_codevector_dim\": 256,\n  \"tdnn_dilation\": [\n    1,\n    2,\n    3,\n    1,\n    1\n  ],\n  \"tdnn_dim\": [\n    512,\n    512,\n    512,\n    512,\n    1500\n  ],\n  \"tdnn_kernel\": [\n    5,\n    3,\n    3,\n    1,\n    1\n  ],\n  \"transformers_version\": \"4.51.1\",\n  \"use_weighted_layer_sum\": false,\n  \"vocab_size\": 32,\n  \"xvector_output_dim\": 512\n}\n\n","output_type":"stream"},{"name":"stdout","text":"Model config loaded and vocab_size updated.\n","output_type":"stream"}],"execution_count":319},{"cell_type":"code","source":"if not model_config.do_stable_layer_norm or model_config.feat_extract_norm != \"layer\":\n    raise ValueError(\"Only supports stable layer norm with `feat_extract_norm='layer'`\")\n\nmodel = Wav2Vec2ForPreTraining(model_config)\n\nif config.gradient_checkpointing:\n    model.gradient_checkpointing_enable()\n\nmask_time_prob = config.mask_time_prob or model_config.mask_time_prob\nmask_time_length = config.mask_time_length or model_config.mask_time_length","metadata":{"id":"nvYIQ7s13s8p","trusted":true,"execution":{"iopub.status.busy":"2025-05-11T19:14:06.189555Z","iopub.execute_input":"2025-05-11T19:14:06.189988Z","iopub.status.idle":"2025-05-11T19:14:07.497020Z","shell.execute_reply.started":"2025-05-11T19:14:06.189966Z","shell.execute_reply":"2025-05-11T19:14:07.496278Z"}},"outputs":[],"execution_count":320},{"cell_type":"code","source":"data_collator = DataCollatorForWav2Vec2Pretraining(\n    model=model,\n    feature_extractor=feature_extractor,\n    config = config\n)\n\ntrain_dataloader = DataLoader(\n    train_dataset,\n    shuffle=True,\n    collate_fn=data_collator,\n    batch_size=config.per_device_train_batch_size,\n)\neval_dataloader = DataLoader(\n    eval_dataset,\n    collate_fn=data_collator,\n    batch_size=config.per_device_eval_batch_size,\n)\n\noptimizer = torch.optim.AdamW(\n    model.parameters(),\n    lr=config.learning_rate,\n    betas=(config.adam_beta1, config.adam_beta2),\n    eps=config.adam_epsilon,\n)","metadata":{"id":"LXxRZ_zS3uIs","trusted":true,"execution":{"iopub.status.busy":"2025-05-11T19:14:09.434800Z","iopub.execute_input":"2025-05-11T19:14:09.435333Z","iopub.status.idle":"2025-05-11T19:14:09.440864Z","shell.execute_reply.started":"2025-05-11T19:14:09.435311Z","shell.execute_reply":"2025-05-11T19:14:09.440117Z"}},"outputs":[],"execution_count":321},{"cell_type":"code","source":"num_training_steps = config.max_train_steps\nlr_scheduler = get_scheduler(\n    config.lr_scheduler_type,\n    optimizer=optimizer,\n    num_warmup_steps=100,\n    num_training_steps=num_training_steps,\n)\n\naccelerator.register_for_checkpointing(lr_scheduler)\n\ntotal_batch_size = config.per_device_train_batch_size * accelerator.num_processes * config.gradient_accumulation_steps\nlogger.info(f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}\")\nlogger.info(f\"  Total optimization steps = {config.max_train_steps}\")","metadata":{"id":"CaixqONzSIqw","trusted":true,"execution":{"iopub.status.busy":"2025-05-11T19:14:12.411920Z","iopub.execute_input":"2025-05-11T19:14:12.412606Z","iopub.status.idle":"2025-05-11T19:14:12.416757Z","shell.execute_reply.started":"2025-05-11T19:14:12.412584Z","shell.execute_reply":"2025-05-11T19:14:12.416142Z"}},"outputs":[],"execution_count":322},{"cell_type":"code","source":"if iter_index > 0:\n    print(\"❇️ [LOAD INFO] LOADING STATE FROM SAVED FILES\")\n    load_path = os.path.join(\"/kaggle/working/TrainingCheckpoints\", \"training_state.pt\")\n    state = torch.load(load_path, map_location=\"cpu\", weights_only=False)\n\n    model.load_state_dict(state[\"model\"])\n    optimizer.load_state_dict(state[\"optimizer\"])\n    lr_scheduler.load_state_dict(state[\"scheduler\"])\n\n    if state.get(\"scaler\") and hasattr(accelerator, \"scaler\"):\n        accelerator.scaler.load_state_dict(state[\"scaler\"])\n\n    epoch = state[\"epoch\"]\n    completed_steps = state[\"completed_steps\"]\n\n    # Restore RNG states\n    random.setstate(state[\"rng_state\"][\"random\"])\n    torch.set_rng_state(state[\"rng_state\"][\"torch\"])\n    if torch.cuda.is_available() and state[\"rng_state\"][\"cuda\"]:\n        torch.cuda.set_rng_state_all(state[\"rng_state\"][\"cuda\"])","metadata":{"id":"_JVf3YUUX3Zp","outputId":"84ff06c0-262c-43b4-f79b-ac5bc856d1b5","trusted":true,"execution":{"iopub.status.busy":"2025-05-11T19:14:14.981022Z","iopub.execute_input":"2025-05-11T19:14:14.981730Z","iopub.status.idle":"2025-05-11T19:14:15.363965Z","shell.execute_reply.started":"2025-05-11T19:14:14.981708Z","shell.execute_reply":"2025-05-11T19:14:15.363169Z"}},"outputs":[{"name":"stdout","text":"❇️ [LOAD INFO] LOADING STATE FROM SAVED FILES\n","output_type":"stream"}],"execution_count":323},{"cell_type":"code","source":"model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n    model, optimizer, train_dataloader, eval_dataloader\n)","metadata":{"id":"5sok3mtuQxGK","trusted":true,"execution":{"iopub.status.busy":"2025-05-11T19:14:18.570150Z","iopub.execute_input":"2025-05-11T19:14:18.570452Z","iopub.status.idle":"2025-05-11T19:14:18.867146Z","shell.execute_reply.started":"2025-05-11T19:14:18.570431Z","shell.execute_reply":"2025-05-11T19:14:18.866614Z"}},"outputs":[],"execution_count":324},{"cell_type":"code","source":"# Initialize trackers\nexperiment_config = vars(config)\naccelerator.init_trackers(\"wav2vec2_pretrain\", experiment_config)","metadata":{"id":"BvOzO_2-STq1","trusted":true,"execution":{"iopub.status.busy":"2025-05-11T19:14:20.495901Z","iopub.execute_input":"2025-05-11T19:14:20.496133Z","iopub.status.idle":"2025-05-11T19:14:20.499887Z","shell.execute_reply.started":"2025-05-11T19:14:20.496116Z","shell.execute_reply":"2025-05-11T19:14:20.499309Z"}},"outputs":[],"execution_count":325},{"cell_type":"code","source":"repo_id = \"wav2vec_bengali_asr_pretraining\"","metadata":{"id":"VhP-IjaaZdnM","trusted":true,"execution":{"iopub.status.busy":"2025-05-11T19:14:23.058695Z","iopub.execute_input":"2025-05-11T19:14:23.059383Z","iopub.status.idle":"2025-05-11T19:14:23.062527Z","shell.execute_reply.started":"2025-05-11T19:14:23.059360Z","shell.execute_reply":"2025-05-11T19:14:23.061765Z"}},"outputs":[],"execution_count":326},{"cell_type":"code","source":"from tqdm import tqdm\nimport torch\nimport numpy as np\n\n# Initialize progress bar\nprogress_bar = tqdm(range(config.max_train_steps), disable=not accelerator.is_local_main_process)\ncompleted_steps = 0\n\nfor epoch in range(config.epoch_per_chunk):\n    model.train()\n    for step, batch in enumerate(train_dataloader):\n        features_shape = batch[\"input_values\"].shape  # [batch_size, raw_audio_len]\n        print(f\"📦 Batch shape: {features_shape}, num_negatives: {model.config.num_negatives}\")\n\n        # 🔍 Estimate downsampled feature length\n        with torch.no_grad():\n            downsampled_len = model._get_feat_extract_output_lengths(torch.tensor(features_shape[1]))\n\n        # Skip the batch if downsampled_len is invalid (too small for negative sampling)\n        if downsampled_len <= model.config.num_negatives:\n            print(f\"⚠️ Skipping batch: Downsampled feature length ({downsampled_len}) <= num_negatives ({model.config.num_negatives})\")\n            continue\n\n        # ✅ Skip if no masked time indices\n        num_losses = batch[\"mask_time_indices\"].sum()\n        if num_losses == 0:\n            print(\"⚠️ Skipping batch due to no masked time indices.\")\n            continue\n\n        # Handle sub_attention_mask if provided\n        sub_attention_mask = batch.pop(\"sub_attention_mask\", None)\n        sub_attention_mask = sub_attention_mask if sub_attention_mask is not None else torch.ones_like(batch[\"mask_time_indices\"])\n\n        try:\n            # ✅ Forward pass\n            outputs = model(**batch)\n        except ValueError as e:\n            if \"high <= 0\" in str(e):\n                print(f\"🚨 Critical error in negative sampling - downsampled_len: {downsampled_len}, num_negatives: {model.config.num_negatives}\")\n                print(\"Skipping problematic batch...\")\n                continue\n            raise  # Re-raise unexpected errors\n\n        loss = outputs.loss / config.gradient_accumulation_steps\n        accelerator.backward(loss)\n\n        # ✅ Optimizer Step\n        if (step + 1) % config.gradient_accumulation_steps == 0 or step == len(train_dataloader) - 1:\n            optimizer.step()\n            optimizer.zero_grad()\n\n            if not accelerator.optimizer_step_was_skipped:\n                lr_scheduler.step()\n            else:\n                print(\"⚠️ Skipping optimizer step due to overflow...\")\n\n            progress_bar.update(1)\n            completed_steps += 1\n\n        # Log training information\n        if accelerator.is_local_main_process and (step + 1) % (config.gradient_accumulation_steps * config.logging_steps) == 0:\n            loss_value = (loss * config.gradient_accumulation_steps) / batch[\"mask_time_indices\"].sum()\n            train_logs = {\n                \"loss\": loss_value.item(),\n                \"%_mask_idx\": (num_losses / sub_attention_mask.sum()).item(),\n            }\n            print(f\"📢 Training Logs: {train_logs}\")\n            \n        # Save the model periodically\n        if (step + 1) % (config.gradient_accumulation_steps * config.saving_steps) == 0:\n            if config.output_dir is not None:\n                unwrapped_model = accelerator.unwrap_model(model)\n                unwrapped_model.save_pretrained(config.output_dir, is_main_process=accelerator.is_main_process)\n\n        # Stop training if max steps are reached\n        if completed_steps >= config.max_train_steps:\n            break\n\n    # Validation Phase\n    model.eval()\n    val_logs = {\"val_loss\": 0, \"val_num_losses\": 0}\n    for step, batch in enumerate(eval_dataloader):\n        with torch.no_grad():\n            batch.pop(\"sub_attention_mask\", None)\n            outputs = model(**batch)\n\n        val_logs[\"val_loss\"] += outputs.loss.item()\n        val_logs[\"val_num_losses\"] += batch[\"mask_time_indices\"].sum().item()\n\n    # Summing over devices in multi-processing if needed\n    if accelerator.num_processes > 1:\n        val_logs = {k: accelerator.gather_for_metrics(v).sum() for k, v in val_logs.items()}\n\n    avg_val_loss = val_logs[\"val_loss\"] / val_logs[\"val_num_losses\"] if val_logs[\"val_num_losses\"] > 0 else float('nan')\n    print(f\"📢 Validation Logs: {{'val_loss': {avg_val_loss:.4f}}}\")\n\n    # Save model after validation\n    if config.output_dir:\n        unwrapped_model = accelerator.unwrap_model(model)\n        unwrapped_model.save_pretrained(config.output_dir, is_main_process=accelerator.is_main_process)\n\n# Final report\nprint(f\"\\n🎉 Training completed!\")\nprint(f\"Total completed steps: {completed_steps}\")\n","metadata":{"id":"lFeW6e-PSXLk","trusted":true,"execution":{"iopub.status.busy":"2025-05-11T19:14:25.280852Z","iopub.execute_input":"2025-05-11T19:14:25.281108Z"}},"outputs":[{"name":"stderr","text":"\n100%|██████████| 32/32 [08:27<00:00, 15.86s/it]\n","output_type":"stream"},{"name":"stdout","text":"📦 Batch shape: torch.Size([8, 160000]), num_negatives: 100\n📦 Batch shape: torch.Size([8, 160000]), num_negatives: 100\n📦 Batch shape: torch.Size([8, 160000]), num_negatives: 100\n📦 Batch shape: torch.Size([8, 160000]), num_negatives: 100\n","output_type":"stream"},{"name":"stderr","text":"\n  3%|▎         | 1/32 [00:12<06:42, 12.98s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"📢 Training Logs: {'loss': 0.09968750178813934, '%_mask_idx': 0.5126676559448242}\n📦 Batch shape: torch.Size([8, 160000]), num_negatives: 100\n📦 Batch shape: torch.Size([8, 160000]), num_negatives: 100\n📦 Batch shape: torch.Size([8, 160000]), num_negatives: 100\n📦 Batch shape: torch.Size([8, 160000]), num_negatives: 100\n","output_type":"stream"},{"name":"stderr","text":"\n  6%|▋         | 2/32 [00:24<06:10, 12.36s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"📢 Training Logs: {'loss': 0.09968749433755875, '%_mask_idx': 0.4783802330493927}\n📦 Batch shape: torch.Size([8, 160000]), num_negatives: 100\n📦 Batch shape: torch.Size([8, 160000]), num_negatives: 100\n📦 Batch shape: torch.Size([8, 160000]), num_negatives: 100\n📦 Batch shape: torch.Size([8, 160000]), num_negatives: 100\n","output_type":"stream"},{"name":"stderr","text":"\n  9%|▉         | 3/32 [00:36<05:53, 12.17s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"📢 Training Logs: {'loss': 0.09968750178813934, '%_mask_idx': 0.5236280560493469}\n📦 Batch shape: torch.Size([8, 160000]), num_negatives: 100\n📦 Batch shape: torch.Size([8, 160000]), num_negatives: 100\n📦 Batch shape: torch.Size([8, 160000]), num_negatives: 100\n📦 Batch shape: torch.Size([8, 160000]), num_negatives: 100\n","output_type":"stream"},{"name":"stderr","text":"\n 12%|█▎        | 4/32 [00:48<05:36, 12.02s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"📢 Training Logs: {'loss': 0.09968749433755875, '%_mask_idx': 0.4879603385925293}\n📦 Batch shape: torch.Size([8, 160000]), num_negatives: 100\n📦 Batch shape: torch.Size([8, 160000]), num_negatives: 100\n📦 Batch shape: torch.Size([8, 160000]), num_negatives: 100\n📦 Batch shape: torch.Size([8, 160000]), num_negatives: 100\n","output_type":"stream"},{"name":"stderr","text":"\n 16%|█▌        | 5/32 [01:00<05:21, 11.90s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"📢 Training Logs: {'loss': 0.09968750923871994, '%_mask_idx': 0.49815770983695984}\n📦 Batch shape: torch.Size([8, 160000]), num_negatives: 100\n📦 Batch shape: torch.Size([8, 160000]), num_negatives: 100\n📦 Batch shape: torch.Size([8, 160000]), num_negatives: 100\n📦 Batch shape: torch.Size([8, 160000]), num_negatives: 100\n","output_type":"stream"},{"name":"stderr","text":"\n 19%|█▉        | 6/32 [01:11<05:07, 11.82s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"📢 Training Logs: {'loss': 0.09968749433755875, '%_mask_idx': 0.48273274302482605}\n📦 Batch shape: torch.Size([8, 160000]), num_negatives: 100\n📦 Batch shape: torch.Size([8, 160000]), num_negatives: 100\n📦 Batch shape: torch.Size([8, 160000]), num_negatives: 100\n📦 Batch shape: torch.Size([8, 160000]), num_negatives: 100\n","output_type":"stream"},{"name":"stderr","text":"\n 22%|██▏       | 7/32 [01:23<04:54, 11.78s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"📢 Training Logs: {'loss': 0.09968750178813934, '%_mask_idx': 0.5010814666748047}\n📦 Batch shape: torch.Size([8, 160000]), num_negatives: 100\n📦 Batch shape: torch.Size([8, 160000]), num_negatives: 100\n📦 Batch shape: torch.Size([8, 160000]), num_negatives: 100\n📦 Batch shape: torch.Size([8, 160000]), num_negatives: 100\n","output_type":"stream"},{"name":"stderr","text":"\n 25%|██▌       | 8/32 [01:35<04:42, 11.77s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"📢 Training Logs: {'loss': 0.09968750178813934, '%_mask_idx': 0.5042405724525452}\n📦 Batch shape: torch.Size([8, 160000]), num_negatives: 100\n📦 Batch shape: torch.Size([8, 160000]), num_negatives: 100\n📦 Batch shape: torch.Size([8, 160000]), num_negatives: 100\n📦 Batch shape: torch.Size([8, 160000]), num_negatives: 100\n","output_type":"stream"},{"name":"stderr","text":"\n 28%|██▊       | 9/32 [01:47<04:31, 11.79s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"📢 Training Logs: {'loss': 0.09968750178813934, '%_mask_idx': 0.49531981348991394}\n📦 Batch shape: torch.Size([8, 160000]), num_negatives: 100\n📦 Batch shape: torch.Size([8, 160000]), num_negatives: 100\n📦 Batch shape: torch.Size([8, 160000]), num_negatives: 100\n📦 Batch shape: torch.Size([8, 160000]), num_negatives: 100\n","output_type":"stream"},{"name":"stderr","text":"\n 31%|███▏      | 10/32 [01:59<04:19, 11.81s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"📢 Training Logs: {'loss': 0.09968750178813934, '%_mask_idx': 0.49167823791503906}\n📦 Batch shape: torch.Size([8, 160000]), num_negatives: 100\n📦 Batch shape: torch.Size([8, 160000]), num_negatives: 100\n📦 Batch shape: torch.Size([8, 160000]), num_negatives: 100\n📦 Batch shape: torch.Size([8, 160000]), num_negatives: 100\n","output_type":"stream"},{"name":"stderr","text":"\n 34%|███▍      | 11/32 [02:10<04:07, 11.80s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"📢 Training Logs: {'loss': 0.09968749433755875, '%_mask_idx': 0.4720914661884308}\n📦 Batch shape: torch.Size([8, 160000]), num_negatives: 100\n📦 Batch shape: torch.Size([8, 160000]), num_negatives: 100\n📦 Batch shape: torch.Size([8, 160000]), num_negatives: 100\n📦 Batch shape: torch.Size([8, 160000]), num_negatives: 100\n","output_type":"stream"},{"name":"stderr","text":"\n 38%|███▊      | 12/32 [02:22<03:56, 11.81s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"📢 Training Logs: {'loss': 0.09968750178813934, '%_mask_idx': 0.5103550553321838}\n📦 Batch shape: torch.Size([8, 160000]), num_negatives: 100\n📦 Batch shape: torch.Size([8, 160000]), num_negatives: 100\n📦 Batch shape: torch.Size([8, 160000]), num_negatives: 100\n📦 Batch shape: torch.Size([8, 160000]), num_negatives: 100\n","output_type":"stream"},{"name":"stderr","text":"\n 41%|████      | 13/32 [02:34<03:43, 11.79s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"📢 Training Logs: {'loss': 0.09968749433755875, '%_mask_idx': 0.5044529438018799}\n📦 Batch shape: torch.Size([8, 160000]), num_negatives: 100\n📦 Batch shape: torch.Size([8, 160000]), num_negatives: 100\n📦 Batch shape: torch.Size([8, 160000]), num_negatives: 100\n📦 Batch shape: torch.Size([8, 160000]), num_negatives: 100\n","output_type":"stream"},{"name":"stderr","text":"\n 44%|████▍     | 14/32 [02:46<03:31, 11.77s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"📢 Training Logs: {'loss': 0.09968750178813934, '%_mask_idx': 0.4775928258895874}\n📦 Batch shape: torch.Size([8, 160000]), num_negatives: 100\n📦 Batch shape: torch.Size([8, 160000]), num_negatives: 100\n📦 Batch shape: torch.Size([8, 160000]), num_negatives: 100\n📦 Batch shape: torch.Size([8, 160000]), num_negatives: 100\n","output_type":"stream"},{"name":"stderr","text":"\n 47%|████▋     | 15/32 [02:57<03:20, 11.78s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"📢 Training Logs: {'loss': 0.09968750178813934, '%_mask_idx': 0.4808835983276367}\n📦 Batch shape: torch.Size([8, 160000]), num_negatives: 100\n📦 Batch shape: torch.Size([8, 160000]), num_negatives: 100\n📦 Batch shape: torch.Size([8, 160000]), num_negatives: 100\n📦 Batch shape: torch.Size([8, 160000]), num_negatives: 100\n","output_type":"stream"},{"name":"stderr","text":"\n 50%|█████     | 16/32 [03:09<03:08, 11.76s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"📢 Training Logs: {'loss': 0.09968750178813934, '%_mask_idx': 0.5047549605369568}\n📦 Batch shape: torch.Size([8, 160000]), num_negatives: 100\n📦 Batch shape: torch.Size([8, 160000]), num_negatives: 100\n📦 Batch shape: torch.Size([8, 160000]), num_negatives: 100\n📦 Batch shape: torch.Size([8, 160000]), num_negatives: 100\n","output_type":"stream"},{"name":"stderr","text":"\n 53%|█████▎    | 17/32 [03:21<02:56, 11.76s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"📢 Training Logs: {'loss': 0.09968750178813934, '%_mask_idx': 0.48964595794677734}\n📦 Batch shape: torch.Size([8, 160000]), num_negatives: 100\n📦 Batch shape: torch.Size([8, 160000]), num_negatives: 100\n📦 Batch shape: torch.Size([8, 160000]), num_negatives: 100\n📦 Batch shape: torch.Size([8, 160000]), num_negatives: 100\n","output_type":"stream"},{"name":"stderr","text":"\n 56%|█████▋    | 18/32 [03:33<02:44, 11.77s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"📢 Training Logs: {'loss': 0.09968750178813934, '%_mask_idx': 0.47256767749786377}\n📦 Batch shape: torch.Size([8, 160000]), num_negatives: 100\n📦 Batch shape: torch.Size([8, 160000]), num_negatives: 100\n📦 Batch shape: torch.Size([8, 160000]), num_negatives: 100\n📦 Batch shape: torch.Size([8, 160000]), num_negatives: 100\n","output_type":"stream"},{"name":"stderr","text":"\n 59%|█████▉    | 19/32 [03:45<02:33, 11.77s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"📢 Training Logs: {'loss': 0.09968750178813934, '%_mask_idx': 0.5051395297050476}\n📦 Batch shape: torch.Size([8, 160000]), num_negatives: 100\n📦 Batch shape: torch.Size([8, 160000]), num_negatives: 100\n📦 Batch shape: torch.Size([8, 160000]), num_negatives: 100\n📦 Batch shape: torch.Size([8, 160000]), num_negatives: 100\n","output_type":"stream"},{"name":"stderr","text":"\n 62%|██████▎   | 20/32 [03:56<02:21, 11.78s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"📢 Training Logs: {'loss': 0.09968750178813934, '%_mask_idx': 0.49131694436073303}\n📦 Batch shape: torch.Size([8, 160000]), num_negatives: 100\n📦 Batch shape: torch.Size([8, 160000]), num_negatives: 100\n📦 Batch shape: torch.Size([8, 160000]), num_negatives: 100\n📦 Batch shape: torch.Size([8, 160000]), num_negatives: 100\n","output_type":"stream"},{"name":"stderr","text":"\n 66%|██████▌   | 21/32 [04:08<02:09, 11.76s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"📢 Training Logs: {'loss': 0.09968749433755875, '%_mask_idx': 0.4420391023159027}\n📦 Batch shape: torch.Size([8, 160000]), num_negatives: 100\n📦 Batch shape: torch.Size([8, 160000]), num_negatives: 100\n📦 Batch shape: torch.Size([8, 160000]), num_negatives: 100\n📦 Batch shape: torch.Size([8, 160000]), num_negatives: 100\n","output_type":"stream"},{"name":"stderr","text":"\n 69%|██████▉   | 22/32 [04:20<01:57, 11.76s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"📢 Training Logs: {'loss': 0.09968750178813934, '%_mask_idx': 0.49575069546699524}\n📦 Batch shape: torch.Size([8, 160000]), num_negatives: 100\n📦 Batch shape: torch.Size([8, 160000]), num_negatives: 100\n📦 Batch shape: torch.Size([8, 160000]), num_negatives: 100\n📦 Batch shape: torch.Size([8, 160000]), num_negatives: 100\n","output_type":"stream"},{"name":"stderr","text":"\n 72%|███████▏  | 23/32 [04:32<01:45, 11.76s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"📢 Training Logs: {'loss': 0.09968750923871994, '%_mask_idx': 0.4662402272224426}\n📦 Batch shape: torch.Size([8, 160000]), num_negatives: 100\n📦 Batch shape: torch.Size([8, 160000]), num_negatives: 100\n📦 Batch shape: torch.Size([8, 160000]), num_negatives: 100\n📦 Batch shape: torch.Size([8, 160000]), num_negatives: 100\n","output_type":"stream"},{"name":"stderr","text":"\n 75%|███████▌  | 24/32 [04:43<01:34, 11.77s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"📢 Training Logs: {'loss': 0.09968750923871994, '%_mask_idx': 0.48700815439224243}\n📦 Batch shape: torch.Size([8, 160000]), num_negatives: 100\n📦 Batch shape: torch.Size([8, 160000]), num_negatives: 100\n📦 Batch shape: torch.Size([8, 160000]), num_negatives: 100\n📦 Batch shape: torch.Size([8, 160000]), num_negatives: 100\n","output_type":"stream"},{"name":"stderr","text":"\n 78%|███████▊  | 25/32 [04:55<01:22, 11.77s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"📢 Training Logs: {'loss': 0.09968749433755875, '%_mask_idx': 0.513383686542511}\n📦 Batch shape: torch.Size([8, 160000]), num_negatives: 100\n📦 Batch shape: torch.Size([8, 160000]), num_negatives: 100\n📦 Batch shape: torch.Size([8, 160000]), num_negatives: 100\n📦 Batch shape: torch.Size([8, 160000]), num_negatives: 100\n","output_type":"stream"},{"name":"stderr","text":"\n 81%|████████▏ | 26/32 [05:07<01:10, 11.78s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"📢 Training Logs: {'loss': 0.09968750178813934, '%_mask_idx': 0.509144127368927}\n📦 Batch shape: torch.Size([8, 160000]), num_negatives: 100\n📦 Batch shape: torch.Size([8, 160000]), num_negatives: 100\n📦 Batch shape: torch.Size([8, 160000]), num_negatives: 100\n📦 Batch shape: torch.Size([8, 160000]), num_negatives: 100\n","output_type":"stream"},{"name":"stderr","text":"\n 84%|████████▍ | 27/32 [05:19<00:58, 11.77s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"📢 Training Logs: {'loss': 0.09968750178813934, '%_mask_idx': 0.5167785286903381}\n📦 Batch shape: torch.Size([8, 160000]), num_negatives: 100\n📦 Batch shape: torch.Size([8, 160000]), num_negatives: 100\n📦 Batch shape: torch.Size([8, 160000]), num_negatives: 100\n📦 Batch shape: torch.Size([8, 160000]), num_negatives: 100\n","output_type":"stream"},{"name":"stderr","text":"\n 88%|████████▊ | 28/32 [05:31<00:47, 11.79s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"📢 Training Logs: {'loss': 0.09968750178813934, '%_mask_idx': 0.506373941898346}\n","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"#### Save training states","metadata":{"id":"88ArMA8cY9LF"}},{"cell_type":"code","source":"import os\n\n# ✅ Set Kaggle working directory for saving checkpoints and iteration index\nsave_path = \"/kaggle/working/TrainingCheckpoints\"\nos.makedirs(save_path, exist_ok=True)\n\n# ✅ Save the full training state (model, optimizer, scheduler, etc.)\nstate = {\n    \"epoch\": config.epoch_per_chunk,\n    \"completed_steps\": completed_steps,\n    \"model\": accelerator.unwrap_model(model).state_dict(),\n    \"optimizer\": optimizer.state_dict(),\n    \"scheduler\": lr_scheduler.state_dict(),\n    \"scaler\": accelerator.scaler.state_dict() if hasattr(accelerator, \"scaler\") and accelerator.scaler else None,\n    \"config\": config,\n    \"rng_state\": {\n        \"random\": random.getstate(),\n        \"torch\": torch.get_rng_state(),\n        \"cuda\": torch.cuda.get_rng_state_all() if torch.cuda.is_available() else None,\n    }\n}\n\n# ✅ Save checkpoint to the Kaggle working directory\naccelerator.save(state, os.path.join(save_path, \"training_state.pt\"))\n\n# ✅ Save the model to the save_path directory\nunwrapped_model = accelerator.unwrap_model(model)\nunwrapped_model.save_pretrained(save_path)\n\n# ✅ Save the iteration index to track training progress\nwith open(os.path.join(save_path, \"iter_index.txt\"), \"w\") as f:\n    f.write(str(iter_index + 1))\n\nprint(f\"❇️ [LOAD INFO] Finished training on chunk {iter_index + 1}. Ready for next chunk.\")\nprint(f\"✅ Saved checkpoint, model, and iter_index to: {save_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T19:13:01.247051Z","iopub.execute_input":"2025-05-11T19:13:01.247603Z","iopub.status.idle":"2025-05-11T19:13:04.613908Z","shell.execute_reply.started":"2025-05-11T19:13:01.247580Z","shell.execute_reply":"2025-05-11T19:13:04.613139Z"}},"outputs":[{"name":"stderr","text":"Configuration saved in /kaggle/working/TrainingCheckpoints/config.json\nModel weights saved in /kaggle/working/TrainingCheckpoints/model.safetensors\n","output_type":"stream"},{"name":"stdout","text":"❇️ [LOAD INFO] Finished training on chunk 35. Ready for next chunk.\n✅ Saved checkpoint, model, and iter_index to: /kaggle/working/TrainingCheckpoints\n","output_type":"stream"}],"execution_count":308},{"cell_type":"code","source":"import shutil\nfrom IPython.display import FileLink\n\n# Step 1: Zip the contents of the directory\nshutil.make_archive('/kaggle/working/all_files', 'zip', '/kaggle/working/')\n\n# Step 2: Provide a download link\nFileLink('/kaggle/working/all_files.zip')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T14:05:24.779364Z","iopub.execute_input":"2025-05-11T14:05:24.780031Z","iopub.status.idle":"2025-05-11T14:06:57.361743Z","shell.execute_reply.started":"2025-05-11T14:05:24.780002Z","shell.execute_reply":"2025-05-11T14:06:57.361067Z"}},"outputs":[{"execution_count":150,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/all_files.zip","text/html":"<a href='/kaggle/working/all_files.zip' target='_blank'>/kaggle/working/all_files.zip</a><br>"},"metadata":{}}],"execution_count":150}]}